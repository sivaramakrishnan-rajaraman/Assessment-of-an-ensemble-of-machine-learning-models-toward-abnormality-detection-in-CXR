{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates an ensemble from a selection of models listed: a) customized CNN, b) pretrained VGG16 initialized with ImageNet weights and trained from the scratch, c) Pretrained VGG19 model with ImageNet weights freezed and trained as a classifier, d) Local Binary patterns (LBP)/SVM and e) Histogram of Oriented Gradients (HOG)/SVM classifier. The resulstant ensemble is used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from keras.models import Model, Input\n",
    "import os\n",
    "import statistics\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, SeparableConv2D, Dense, Average, BatchNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from evaluation import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn import preprocessing\n",
    "from skimage import feature\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loading the training data\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "train_data_dir = 'cxr/train'\n",
    "validation_data_dir = 'cxr/validation'\n",
    "epochs = 100\n",
    "batch_size = 16 #in powers of 2 for optimal GPU computation\n",
    "num_classes= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the models work with the data of the same shape, \n",
    "#we define a single input layer that will be used by every model.\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "model_input = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% declaring image data generators, make sure to delcare shuffle=False for validation and testing\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=2,\n",
    "      width_shift_range=0.1,\n",
    "      height_shift_range=0.1,\n",
    "      shear_range=0.1,\n",
    "      zoom_range=0.5,\n",
    "      horizontal_flip=False,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# Note that the validation data and test data should not be augmented!\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',shuffle=False)\n",
    "\n",
    "#identify the number of samples\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "train_generator.class_indices\n",
    "validation_generator.class_indices\n",
    "\n",
    "#true labels\n",
    "Y_test=validation_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% assign class weights to balance model training and penalize over-represented classes\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% define custom model and instantiate it\n",
    "\n",
    "def custom_cnn(model_input):\n",
    "    x = BatchNormalization()(model_input)\n",
    "    x = SeparableConv2D(64, (5, 5), padding='same', activation='relu', name = 'sepconv1')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), name = 'maxpool1')(x)\n",
    "    x = Dropout(0.25, name = 'conv_dropout1')(x)\n",
    "    x = BatchNormalization(name = 'custom_batchnorm1')(x)\n",
    "    x = SeparableConv2D(128, (5, 5), padding='same', activation='relu', name = 'sepconv2')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name = 'maxpool2')(x)\n",
    "    x = Dropout(0.25, name = 'conv_dropout2')(x)\n",
    "    x = BatchNormalization(name = 'custom_batchnorm2')(x)\n",
    "    x = SeparableConv2D(256, (5, 5), padding='same', activation='relu', name = 'sepconv3')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), name = 'maxpool3')(x)\n",
    "    x = Dropout(0.25, name = 'conv_dropout3')(x)\n",
    "    x = GlobalAveragePooling2D(name = 'custom_GAP')(x)\n",
    "    x = Dense(256, activation='relu', name = 'custom_dense1')(x)\n",
    "    x = Dropout(0.5, name = 'custom_dropout1')(x)\n",
    "    x = Dense(num_classes, activation='softmax', name = 'custom_dense2')(x)\n",
    "    model = Model(inputs=model_input, outputs=x, name='custom_cnn')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "custom_model = custom_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "custom_model.summary()\n",
    "\n",
    "#plot the model\n",
    "plot_model(custom_model, to_file='custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Each model is compiled and trained using the same parameters. \n",
    "\n",
    "def compile_and_train(model, num_epochs): \n",
    "    \n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "    filepath = 'weights/' + model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                                 save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "    tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "    callbacks_list = [checkpoint, tensor_board]\n",
    "    history = model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size, verbose=1)                      \n",
    "    return history\n",
    "\n",
    "def evaluate_accuracy(model):\n",
    "\n",
    "    y_pred = model.predict_generator(validation_generator, nb_validation_samples/batch_size, workers=1)\n",
    "    accuracy = accuracy_score(Y_test,y_pred.argmax(axis=-1))\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_mse(model):\n",
    "\n",
    "    y_pred = model.predict_generator(validation_generator, nb_validation_samples/batch_size, workers=1)\n",
    "    mse = mean_squared_error(Y_test,y_pred.argmax(axis=-1))\n",
    "    return mse\n",
    "\n",
    "def evaluate_msle(model):\n",
    "\n",
    "    y_pred = model.predict_generator(validation_generator, nb_validation_samples/batch_size, workers=1)\n",
    "    msle = mean_squared_log_error(Y_test,y_pred.argmax(axis=-1))  \n",
    "    return msle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the custom model\n",
    "    \n",
    "_ = compile_and_train(custom_model, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model by loading the best weights and calculating the accuracy and errors on the test set.\n",
    "\n",
    "custom_model.load_weights('weights/custom_cnn.01-0.5267.h5') #path to the best weights stored\n",
    "\n",
    "print('The accuracy of the Custom model is: ', evaluate_accuracy(custom_model))\n",
    "print('The Mean Squared Error of the Custom model is: ', evaluate_mse(custom_model))\n",
    "print('The Mean Squared Log Error of the Custom model is: ', evaluate_msle(custom_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "custom_y_pred = custom_model.predict_generator(validation_generator, nb_validation_samples/batch_size, workers=1)\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "print(classification_report(Y_test,custom_y_pred.argmax(axis=-1),target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for custom model without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test, custom_y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "fig=plt.figure(figsize=(15,10), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "lw = 1 \n",
    "plt.plot(fpr[1], tpr[1], color='red',\n",
    "         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristics for Custom model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% define the second model : VGG16 \n",
    "\n",
    "def vgg16_cnn(model_input):\n",
    "    vgg16_cnn = VGG16(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    vgg16_cnn = Model(inputs=vgg16_cnn.input, outputs=vgg16_cnn.get_layer('block5_conv3').output)\n",
    "    x = vgg16_cnn.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=vgg16_cnn.input, outputs=predictions, name='vgg16_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "vgg16_custom_model = vgg16_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "vgg16_custom_model.summary()\n",
    "\n",
    "#plot model\n",
    "plot_model(vgg16_custom_model, to_file='vgg16_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile and train the model\n",
    "_ = compile_and_train(vgg16_custom_model, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the accuracy and errors on the test set\n",
    "\n",
    "vgg16_custom_model.load_weights('weights/vgg16_custom.01-0.9817.h5')\n",
    "\n",
    "#renaming the model layers to prevent issues while doing model ensemble\n",
    "vgg16_custom_model.get_layer(name='block1_conv1').name='block1_conv1VGG'  \n",
    "vgg16_custom_model.get_layer(name='block1_conv2').name='block1_conv2VGG' \n",
    "vgg16_custom_model.get_layer(name='block2_conv1').name='block2_conv1VGG' \n",
    "vgg16_custom_model.get_layer(name='block2_conv2').name='block2_conv2VGG' \n",
    "vgg16_custom_model.get_layer(name='block3_conv1').name='block3_conv1VGG' \n",
    "vgg16_custom_model.get_layer(name='block3_conv2').name='block3_conv2VGG' \n",
    "vgg16_custom_model.get_layer(name='block3_conv3').name='block3_conv3VGG' \n",
    "vgg16_custom_model.get_layer(name='block4_conv1').name='block4_conv1VGG' \n",
    "vgg16_custom_model.get_layer(name='block4_conv2').name='block4_conv2VGG' \n",
    "vgg16_custom_model.get_layer(name='block4_conv3').name='block4_conv3VGG' \n",
    "vgg16_custom_model.get_layer(name='block5_conv1').name='block5_conv1VGG' \n",
    "vgg16_custom_model.get_layer(name='block5_conv2').name='block5_conv2VGG' \n",
    "vgg16_custom_model.get_layer(name='block5_conv3').name='block5_conv3VGG' \n",
    "vgg16_custom_model.get_layer(name='block1_pool').name='block1_poolVGG' \n",
    "vgg16_custom_model.get_layer(name='block2_pool').name='block2_poolVGG' \n",
    "vgg16_custom_model.get_layer(name='block3_pool').name='block3_poolVGG' \n",
    "vgg16_custom_model.get_layer(name='block4_pool').name='block4_poolVGG' \n",
    "\n",
    "#display model summary\n",
    "vgg16_custom_model.summary()\n",
    "\n",
    "print('The accuracy of the VGG16 model is: ', evaluate_accuracy(vgg16_custom_model))\n",
    "print('The Mean Squared Error of the VGG16 model is: ', evaluate_mse(vgg16_custom_model))\n",
    "print('The Mean Squared Log Error of the VGG16 model is: ', evaluate_msle(vgg16_custom_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "vgg16_y_pred = vgg16_custom_model.predict_generator(validation_generator, nb_validation_samples/batch_size, workers=1)\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "print(classification_report(Y_test,vgg16_y_pred.argmax(axis=-1),target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,vgg16_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for vgg16 without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test, vgg16_y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "fig=plt.figure(figsize=(15,10), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "lw = 1 \n",
    "plt.plot(fpr[1], tpr[1], color='red',\n",
    "         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristics for VGG16 model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% The third model is the xception model to be trained as a classifier\n",
    "\n",
    "def vgg19_cnn(model_input):\n",
    "    vgg19_cnn = VGG19(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = vgg19_cnn.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=vgg19_cnn.input, outputs=predictions, name='vgg19_custom')\n",
    "    #freeze the training layers and train only the classifier\n",
    "    for layer in vgg19_cnn.layers:\n",
    "        layer.trainable = False   \n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "vgg19_custom_model = vgg19_cnn(model_input)\n",
    "\n",
    "#plot model summary\n",
    "vgg19_custom_model.summary()\n",
    "#plot_model(vgg19_custom_model, to_file='vgg19_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile and train the model\n",
    "_ = compile_and_train(vgg19_custom_model, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the accuracy and errors on the test set\n",
    "vgg19_custom_model.load_weights('weights/vgg19_custom.01-0.7652.h5')\n",
    "\n",
    "#renaming the model layers to prevent issues while doing model ensemble\n",
    "vgg19_custom_model.get_layer(name='block1_conv1').name='block1_conv1VGG19'  \n",
    "vgg19_custom_model.get_layer(name='block1_conv2').name='block1_conv2VGG19' \n",
    "vgg19_custom_model.get_layer(name='block2_conv1').name='block2_conv1VGG19' \n",
    "vgg19_custom_model.get_layer(name='block2_conv2').name='block2_conv2VGG19' \n",
    "vgg19_custom_model.get_layer(name='block3_conv1').name='block3_conv1VGG19' \n",
    "vgg19_custom_model.get_layer(name='block3_conv2').name='block3_conv2VGG19' \n",
    "vgg19_custom_model.get_layer(name='block3_conv3').name='block3_conv3VGG19' \n",
    "vgg19_custom_model.get_layer(name='block3_conv4').name='block3_conv4VGG19' \n",
    "vgg19_custom_model.get_layer(name='block4_conv1').name='block4_conv1VGG19' \n",
    "vgg19_custom_model.get_layer(name='block4_conv2').name='block4_conv2VGG19' \n",
    "vgg19_custom_model.get_layer(name='block4_conv3').name='block4_conv3VGG19' \n",
    "vgg19_custom_model.get_layer(name='block4_conv4').name='block4_conv4VGG19' \n",
    "vgg19_custom_model.get_layer(name='block5_conv1').name='block5_conv1VGG19' \n",
    "vgg19_custom_model.get_layer(name='block5_conv2').name='block5_conv2VGG19' \n",
    "vgg19_custom_model.get_layer(name='block5_conv3').name='block5_conv3VGG19' \n",
    "vgg19_custom_model.get_layer(name='block5_conv4').name='block5_conv4VGG19'\n",
    "vgg19_custom_model.get_layer(name='block1_pool').name='block1_poolVGG19' \n",
    "vgg19_custom_model.get_layer(name='block2_pool').name='block2_poolVGG19' \n",
    "vgg19_custom_model.get_layer(name='block3_pool').name='block3_poolVGG19' \n",
    "vgg19_custom_model.get_layer(name='block4_pool').name='block4_poolVGG19' \n",
    "\n",
    "print('The accuracy of the vgg19 model is: ', evaluate_accuracy(vgg19_custom_model))\n",
    "print('The Mean Squared Error of the vgg19 model is: ', evaluate_mse(vgg19_custom_model))\n",
    "print('The Mean Squared Log Error of the vgg19 model is: ', evaluate_msle(vgg19_custom_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "vgg19_y_pred = vgg19_custom_model.predict_generator(validation_generator, nb_validation_samples/batch_size, workers=1)\n",
    "print(vgg19_y_pred.shape)\n",
    "\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "print(classification_report(Y_test,vgg19_y_pred.argmax(axis=-1),target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,vgg19_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for VGG19 without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test, vgg19_y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "fig=plt.figure(figsize=(15,10), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "lw = 1 \n",
    "plt.plot(fpr[1], tpr[1], color='red',\n",
    "         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristics for VGG19 model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local binary patterns (LBP) is a type of visual descriptor used for classification in computer vision. It has been found to be a powerful feature for texture classification; it has further been determined that when LBP is combined with the Histogram of oriented gradients (HOG) descriptor, it improves the detection performance considerably on some datasets.The LBP feature vector is created in the following manner: \n",
    "\n",
    "Divide the examined image into cells (e.g. 16x16 pixels for each cell).\n",
    "\n",
    "For each pixel in a cell, compare the pixel to each of its 8 neighbors (on its left-top, left-middle, left-bottom, right-top, etc.). \n",
    "\n",
    "Follow the pixels along a circle, i.e. clockwise or counter-clockwise.\n",
    "\n",
    "Where the center pixel's value is greater than the neighbor's value, write \"0\". Otherwise, write \"1\". This gives an 8-digit binary number (which is usually converted to decimal for convenience).\n",
    "\n",
    "Compute the histogram, over the cell, of the frequency of each \"number\" occurring (i.e., each combination of which pixels are smaller and which are greater than the center). This histogram can be seen as a 256-dimensional feature vector.\n",
    "Optionally normalize the histogram.\n",
    "\n",
    "Concatenate (normalized) histograms of all cells. This gives a feature vector for the entire window.\n",
    "\n",
    "The feature vector can now be processed using the Support vector machine, extreme learning machines, or some other machine-learning algorithm to classify images. Such classifiers can be used for face recognition or texture analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBinaryPatterns:\n",
    "    def __init__(self, numPoints, radius):\n",
    "        # store the number of points and radius\n",
    "        self.numPoints = numPoints\n",
    "        self.radius = radius\n",
    " \n",
    "    def describe(self, image, eps=1e-7):\n",
    "        # compute the Local Binary Pattern representation\n",
    "        # of the image, and then use the LBP representation\n",
    "        # to build the histogram of patterns\n",
    "        lbp = feature.local_binary_pattern(image, self.numPoints,\n",
    "            self.radius, method=\"uniform\")\n",
    "        (hist, _) = np.histogram(lbp.ravel(),\n",
    "            bins=np.arange(0, self.numPoints + 3),\n",
    "            range=(0, self.numPoints + 2))\n",
    " \n",
    "        # normalize the histogram\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + eps)\n",
    " \n",
    "        # return the histogram of Local Binary Patterns\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% empty list to hold feature vectors and train labels\n",
    "        \n",
    "lbp_train_features = []\n",
    "lbp_train_labels = []\n",
    "\n",
    "# initialize our LocalBinary Pattern descriptor using numPoints and radius.\n",
    "# radius = 3 and numpoints = 8 * radius (=24) works well for this problem\n",
    "\n",
    "desc = LocalBinaryPatterns(24, 3) \n",
    "\n",
    "# loop over the training dataset\n",
    "print (\"[STATUS] Started extracting LBP features....\")\n",
    "data_dir = os.path.join(os.getcwd(),train_data_dir)\n",
    "\n",
    "cur_label = os.listdir(data_dir)\n",
    "cur_label.sort()\n",
    "\n",
    "for label in cur_label:\n",
    "    dirpath = os.path.join(data_dir,label)\n",
    "    i = 1\n",
    "    for file in glob.glob(dirpath + \"/*.png\"):\n",
    "            print (\"Processing Image - {} in {}\".format(i, label))\n",
    "            \n",
    "            # read the training image\n",
    "            image = cv2.imread(file)\n",
    "            \n",
    "            # convert the image to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # extract Local Binary Patterns from the image\n",
    "            lbp_features = desc.describe(gray)\n",
    "\n",
    "            # append the feature vector and label\n",
    "            lbp_train_features.append(lbp_features)\n",
    "            lbp_train_labels.append(label)\n",
    "\n",
    "            # show loop update\n",
    "            i += 1\n",
    "\n",
    "# have a look at the size of our feature vector and labels\n",
    "print (\"Training features: {}\".format(np.array(lbp_train_features).shape))\n",
    "print (\"Training labels: {}\".format(np.array(lbp_train_labels).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% create the classifier\n",
    "print (\"[STATUS] Creating the classifier..\")\n",
    "clf_nusvm_lbp = NuSVC(nu=0.4, kernel='linear', probability=True, class_weight='balanced', random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of nu = 04 and linear kernel gave the best results here. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)). The probability is set to true to compute class probabilities however the function is not available for all classifiers incuding kNN but only SVC and NuSVC has it according to\n",
    "https://stackoverflow.com/questions/15015710/how-can-i-know-probability-of-class-predicted-by-predict-function-in-support-v\n",
    "\n",
    "The trained model can also be stored using pickle like this:\n",
    "\n",
    "lbp_svm_filename = 'LBP_SVM_MODEL.sav'\n",
    "\n",
    "pickle.dump(clf_nusvm_lbp, open(lbp_svm_filename, 'wb'))\n",
    "\n",
    "The saved model can be loaded from disk like this:\n",
    "\n",
    "lbp_svm_model = pickle.load(open(lbp_svm_filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"[STATUS] Fitting data/label to model..\")\n",
    "\n",
    "#create an array of training features and labels\n",
    "lbp_train_features = np.array(lbp_train_features)\n",
    "lbp_train_labels = np.array(lbp_train_labels)\n",
    "\n",
    "#randomize the training data\n",
    "lbp_idxs = np.random.permutation(len(lbp_train_features))\n",
    "lbp_train_features = lbp_train_features[lbp_idxs]\n",
    "lbp_train_labels = lbp_train_labels[lbp_idxs]\n",
    "\n",
    "#fit the classifier\n",
    "clf_nusvm_lbp.fit(lbp_train_features, lbp_train_labels)\n",
    "\n",
    "#save the model to disk\n",
    "lbp_svm_filename = 'LBP_SVM_MODEL.sav'\n",
    "joblib.dump(clf_nusvm_lbp, lbp_svm_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load the saved model and predict on the test data \n",
    "lbp_svm_model = joblib.load(lbp_svm_filename)\n",
    "\n",
    "#load test data\n",
    "data_dir = os.path.join(os.getcwd(),validation_data_dir)\n",
    "\n",
    "lbp_y_pred = []\n",
    "lbp_y_pred1 = []\n",
    "Y_test_lbp = []\n",
    "lbp_class_labels = []\n",
    "\n",
    "cur_label = os.listdir(data_dir)\n",
    "cur_label.sort()\n",
    "\n",
    "for label in cur_label:\n",
    "    dirpath = os.path.join(data_dir,label)\n",
    "    i = 1\n",
    "    lbp_class_labels.append(label)\n",
    "    for file in glob.glob(dirpath + \"/*.png\"): #modify for png\n",
    "            print (\"Processing Image - {} in {}\".format(i, label))\n",
    "            \n",
    "            # read the training image\n",
    "            image = cv2.imread(file)\n",
    "            Y_test_lbp.append(label)\n",
    "            \n",
    "            # convert the image to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # extract LBP features from the image\n",
    "            lbp_test_features = desc.describe(gray)\n",
    "\n",
    "            # evaluate the model and predict label\n",
    "            lbp_prediction = lbp_svm_model.predict(lbp_test_features.reshape(1, -1))[0]\n",
    "            lbp_prediction1 = lbp_svm_model.predict_proba(lbp_test_features.reshape(1, -1))[0]\n",
    "            lbp_y_pred.append(lbp_prediction)\n",
    "            lbp_y_pred1.append(lbp_prediction1)\n",
    "            # show loop update\n",
    "            i += 1\n",
    "            \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(lbp_class_labels)\n",
    "\n",
    "localbinary_y_pred = le.transform(lbp_y_pred)\n",
    "Y_test_lbp = le.transform(Y_test_lbp)\n",
    "localbinary_y_pred1 = np.array(lbp_y_pred1)\n",
    "\n",
    "#print shape to check\n",
    "print(localbinary_y_pred.shape)\n",
    "print(localbinary_y_pred1.shape)\n",
    "print(Y_test_lbp.shape)\n",
    "\n",
    "#save the predictions\n",
    "np.savetxt('localbinary_y_pred.csv',localbinary_y_pred1.argmax(axis=-1),fmt='%i',delimiter = \",\")\n",
    "np.savetxt('Y_test_lbp.csv',Y_test_lbp,fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% measure accuracy\n",
    "\n",
    "LBP_SVM_model_accuracy=accuracy_score(Y_test_lbp,localbinary_y_pred)\n",
    "print(\"The accuracy of LBP SVM model is = \", LBP_SVM_model_accuracy)\n",
    "\n",
    "#plot confusion matrix\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test_lbp,localbinary_y_pred1.argmax(axis=-1),target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test_lbp,localbinary_y_pred1.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for LBP SVM model without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "lbp_fpr = dict()\n",
    "lbp_tpr = dict()\n",
    "lbp_roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    lbp_fpr[i], lbp_tpr[i], _ = roc_curve(Y_test_lbp, localbinary_y_pred1[:,i])\n",
    "    lbp_roc_auc[i] = auc(lbp_fpr[i], lbp_tpr[i])\n",
    "fig=plt.figure(figsize=(15,10), dpi=300)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Major ticks every 0.05, minor ticks every 0.05\n",
    "major_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.05)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "lw = 1 \n",
    "plt.plot(lbp_fpr[1], lbp_tpr[1], color='cyan',\n",
    "         lw=lw, label='ROC curve (area = %0.4f)' % lbp_roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristics for LBP SVM model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate error\n",
    "\n",
    "LBP_SVM_model_mean_squared_error = mean_squared_error(Y_test_lbp,localbinary_y_pred)  \n",
    "LBP_SVM_model_mean_squared_log_error = mean_squared_log_error(Y_test_lbp,localbinary_y_pred)  \n",
    "print(\"The mean squared error of LBP_NUSVMV model is = \", LBP_SVM_model_mean_squared_error)\n",
    "print(\"The mean squared log error of LBP_NUSVMV model is = \",LBP_SVM_model_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts occurrences of gradient orientation in localized portions of an image. This method is similar to that of edge orientation histograms, scale-invariant feature transform descriptors, and shape contexts, but differs in that it is computed on a dense grid of uniformly spaced cells and uses overlapping local contrast normalization for improved accuracy. \n",
    "\n",
    "The essential thought behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions. The image is divided into small connected regions called cells, and for the pixels within each cell, a histogram of gradient directions is compiled. The descriptor is the concatenation of these histograms. \n",
    "\n",
    "For improved accuracy, the local histograms can be contrast-normalized by calculating a measure of the intensity across a larger region of the image, called a block, and then using this value to normalize all cells within the block. This normalization results in better invariance to changes in illumination and shadowing. \n",
    "\n",
    "The HOG descriptor has a few key advantages over other descriptors. Since it operates on local cells, it is invariant to geometric and photometric transformations, except for object orientation. Such changes would only appear in larger spatial regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% HOG feature extraction\n",
    "\n",
    "hog_train_features = []\n",
    "hog_train_labels = []\n",
    "\n",
    "def extract_features_hog(image):\n",
    "    hog_orientation = feature.hog(image, orientations=9, pixels_per_cell=(8,8),\n",
    "                                  cells_per_block=(3,3), visualise=None, transform_sqrt=False, feature_vector=True)\n",
    "    \n",
    "    # take the mean of it and return it\n",
    "    return hog_orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% loop over the training dataset\n",
    "\n",
    "print (\"[STATUS] Started extracting HOG textures..\")\n",
    "data_dir = os.path.join(os.getcwd(),train_data_dir)\n",
    "\n",
    "cur_label = os.listdir(data_dir)\n",
    "cur_label.sort()\n",
    "\n",
    "for label in cur_label:\n",
    "    dirpath = os.path.join(data_dir,label)\n",
    "    i = 1\n",
    "    for file in glob.glob(dirpath + \"/*.png\"): #change for png files\n",
    "            print (\"Processing Image - {} in {}\".format(i, label))\n",
    "            \n",
    "            # read the training image\n",
    "            image = cv2.imread(file)\n",
    "            \n",
    "            # convert the image to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # extract HOG features from the image\n",
    "            hog_features = extract_features_hog(gray)\n",
    "\n",
    "            # append the feature vector and label\n",
    "            hog_train_features.append(hog_features)\n",
    "            hog_train_labels.append(label)\n",
    "\n",
    "            # show loop update\n",
    "            i += 1\n",
    "                \n",
    "# have a look at the size of our feature vector and labels\n",
    "print (\"Training features: {}\".format(np.array(hog_train_features).shape))\n",
    "print (\"Training labels: {}\".format(np.array(hog_train_labels).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% create the classifier\n",
    "print (\"[STATUS] Creating the classifier..\")\n",
    "clf_nusvm_hog = NuSVC(nu=0.4, kernel='linear', probability=True, class_weight='balanced', random_state=9)\n",
    "\n",
    "print (\"[STATUS] Fitting data/label to model..\")\n",
    "#create an array of training features and labels\n",
    "hog_train_features = np.array(hog_train_features)\n",
    "hog_train_labels = np.array(hog_train_labels)\n",
    "\n",
    "#randomize the training data to make it robust\n",
    "hog_idxs = np.random.permutation(len(hog_train_features))\n",
    "hog_train_features = hog_train_features[hog_idxs]\n",
    "hog_train_labels = hog_train_labels[hog_idxs]\n",
    "\n",
    "#fit the classifier\n",
    "clf_nusvm_hog.fit(hog_train_features, hog_train_labels)\n",
    "\n",
    "#save the model to disk\n",
    "hog_svm_filename = 'HOG_SVM_MODEL.sav'\n",
    "joblib.dump(clf_nusvm_hog, hog_svm_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load the saved model and predict on the test data \n",
    "hog_svm_model = joblib.load(hog_svm_filename)\n",
    "\n",
    "# predict on the test data\n",
    "data_dir = os.path.join(os.getcwd(),validation_data_dir)\n",
    "\n",
    "hog_y_pred = []\n",
    "hog_y_pred1 = []\n",
    "Y_test_hog = [] #ground truth labels\n",
    "hog_class_labels = []\n",
    "\n",
    "cur_label = os.listdir(data_dir)\n",
    "cur_label.sort()\n",
    "\n",
    "for label in cur_label:\n",
    "    dirpath = os.path.join(data_dir,label)\n",
    "    i = 1\n",
    "    hog_class_labels.append(label)\n",
    "    for file in glob.glob(dirpath + \"/*.png\"):\n",
    "            print (\"Processing Image - {} in {}\".format(i, label))\n",
    "            \n",
    "            # read the training image\n",
    "            image = cv2.imread(file)\n",
    "            Y_test_hog.append(label)\n",
    "            \n",
    "            # convert the image to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # extract HOG features from the image\n",
    "            hog_test_features = extract_features_hog(gray)\n",
    "            \n",
    "            # make predictions\n",
    "            hog_prediction = hog_svm_model.predict(hog_test_features.reshape(1, -1))[0]\n",
    "            hog_prediction1 = hog_svm_model.predict_proba(hog_test_features.reshape(1, -1))[0]\n",
    "            hog_y_pred.append(hog_prediction)\n",
    "            hog_y_pred1.append(hog_prediction1)\n",
    "            \n",
    "            # show loop update\n",
    "            i += 1\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(hog_class_labels)\n",
    "\n",
    "hog_y_pred = le.transform(hog_y_pred)\n",
    "Y_test_hog = le.transform(Y_test_hog)\n",
    "hog_y_pred1 = np.array(hog_y_pred1)\n",
    "\n",
    "#print shape to check\n",
    "print(hog_y_pred.shape)\n",
    "print(hog_y_pred1.shape)\n",
    "print(Y_test_hog.shape)\n",
    "\n",
    "#save the predictions\n",
    "np.savetxt('hog_y_pred.csv',hog_y_pred1.argmax(axis=-1),fmt='%i',delimiter = \",\")\n",
    "np.savetxt('Y_test_hog.csv',Y_test_hog,fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% measure accuracy\n",
    "\n",
    "hog_svm_model_accuracy=accuracy_score(Y_test_hog,hog_y_pred)\n",
    "print(\"The accuracy of HOG SVM is = \", hog_svm_model_accuracy)\n",
    "\n",
    "#plot confusion matrix\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test_hog,hog_y_pred,target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test_hog,hog_y_pred)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for HOG SVM model without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute roc_auc values\n",
    "\n",
    "hog_fpr = dict()\n",
    "hog_tpr = dict()\n",
    "hog_roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    hog_fpr[i], hog_tpr[i], _ = roc_curve(Y_test_hog, hog_y_pred1[:,i])\n",
    "    hog_roc_auc[i] = auc(hog_fpr[i], hog_tpr[i])\n",
    "fig=plt.figure(figsize=(15,10), dpi=300)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "major_ticks = np.arange(0.0, 1.0, 0.1)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.1)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "lw = 1 \n",
    "plt.plot(hog_fpr[1], hog_tpr[1], color='green',\n",
    "         lw=lw, label='ROC curve (area = %0.4f)' % hog_roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristics for HOG SVM model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate error and print\n",
    "\n",
    "HOG_SVM_model_mean_squared_error = mean_squared_error(Y_test_hog,hog_y_pred)  \n",
    "HOG_SVM_model_mean_squared_log_error = mean_squared_log_error(Y_test_hog,hog_y_pred)  \n",
    "print(\"The mean squared error of HOG_NUSVM model is = \",HOG_SVM_model_mean_squared_error)\n",
    "print(\"The mean squared log error of HOG_NUSVM model is = \",HOG_SVM_model_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy assignment of variable for majority voting\n",
    "\n",
    "y_pred1 = custom_y_pred\n",
    "vgg16_y_pred1 = vgg16_y_pred\n",
    "vgg19_y_pred1 = vgg19_y_pred\n",
    "\n",
    "print(\"The shape of custom model prediction y_pred is  = \", y_pred1.shape)\n",
    "print(\"The shape of VGG16 model prediction vgg16_y_pred is  = \", vgg16_y_pred1.shape)\n",
    "print(\"The shape of VGG19 model prediction vgg19_y_pred is = \", vgg19_y_pred1.shape)\n",
    "print(\"The shape of LBP SVM model prediction localbinary_y_pred is  = \", localbinary_y_pred.shape)\n",
    "print(\"The shape of HOG SVM model prediction hog_y_pred is  = \", hog_y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = y_pred1.argmax(axis=-1)\n",
    "print(y_pred1)\n",
    "vgg16_y_pred1 = vgg16_y_pred1.argmax(axis=-1)\n",
    "print(vgg16_y_pred1)\n",
    "vgg19_y_pred1 = vgg19_y_pred1.argmax(axis=-1)\n",
    "print(vgg19_y_pred1)\n",
    "\n",
    "#max voting begins\n",
    "max_voting_pred = np.array([])\n",
    "for i in range(0,len(validation_generator.filenames)):\n",
    "    max_voting_pred = np.append(max_voting_pred, \n",
    "                                statistics.mode([y_pred1[i], vgg16_y_pred1[i], vgg19_y_pred1[i],\n",
    "                                                 localbinary_y_pred[i], hog_y_pred[i]]))\n",
    "    \n",
    "ensemble_model_max_voting_accuracy = accuracy_score(Y_test,max_voting_pred)\n",
    "print(\"The max voting accuracy of the ensemble model is  = \", ensemble_model_max_voting_accuracy)\n",
    "\n",
    "#plot confusion matrix\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test,max_voting_pred,target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,max_voting_pred)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for Max Voting ensemble without normalization')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#save the predictions\n",
    "np.savetxt('max_voting_y_pred.csv',max_voting_pred,fmt='%i',delimiter = \",\")\n",
    "np.savetxt('Y_test_maxvoting.csv',Y_test,fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate error\n",
    "\n",
    "ensemble_model_maxvoting_mean_squared_error = mean_squared_error(Y_test,max_voting_pred)  \n",
    "ensemble_model_maxvoting_mean_squared_log_error = mean_squared_log_error(Y_test,max_voting_pred)  \n",
    "print(\"The max voting mean squared error of the ensemble model is  = \", ensemble_model_maxvoting_mean_squared_error)\n",
    "print(\"The max voting mean squared log error of the ensemble model is  = \", ensemble_model_maxvoting_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Simple Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% simple averaging\n",
    "\n",
    "average_pred=(custom_y_pred + vgg16_y_pred + vgg19_y_pred + localbinary_y_pred1 + hog_y_pred1)/5\n",
    "ensemble_model_averaging_accuracy = accuracy_score(Y_test,average_pred.argmax(axis=-1))\n",
    "print(\"The averaging accuracy of the ensemble model is  = \", ensemble_model_averaging_accuracy)\n",
    "\n",
    "#plot confusion matrix\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test,average_pred.argmax(axis=-1),target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,average_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for Average Ensemble without normalization')\n",
    "\n",
    "plt.show()\n",
    "#save the predictions\n",
    "np.savetxt('averagring_y_pred.csv',average_pred.argmax(axis=-1),fmt='%i',delimiter = \",\")\n",
    "np.savetxt('Y_test_averaging.csv',Y_test,fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot roc auc\n",
    "\n",
    "avg_fpr = dict()\n",
    "avg_tpr = dict()\n",
    "avg_roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    avg_fpr[i], avg_tpr[i], _ = roc_curve(Y_test, average_pred[:, i])\n",
    "    avg_roc_auc[i] = auc(avg_fpr[i], avg_tpr[i])\n",
    "fig=plt.figure(figsize=(15,10), dpi=300)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "major_ticks = np.arange(0.0, 1.0, 0.1)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.1)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "lw = 1 \n",
    "plt.plot(avg_fpr[1], avg_tpr[1], color='cyan',\n",
    "         lw=lw, label='ROC curve (area = %0.4f)' % avg_roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristics for VGG19 model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate error\n",
    "ensemble_model_averaging_mean_squared_error = mean_squared_error(Y_test,average_pred.argmax(axis=-1))  \n",
    "ensemble_model_averaging_mean_squared_log_error = mean_squared_log_error(Y_test,average_pred.argmax(axis=-1))  \n",
    "print(\"The averaging mean squared error of the ensemble model is  = \", ensemble_model_averaging_mean_squared_error)\n",
    "print(\"The averaging mean squared log error of the ensemble model is  = \", ensemble_model_averaging_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Averaging: This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. With VGG16 having generic knowledge about images, its predictions is given more importance as compared to the other models.Next, VGG19 is given higher weights followed by the other models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average_pred=(custom_y_pred * 0.05 + vgg16_y_pred * 0.4 + vgg19_y_pred * 0.3\n",
    "                       + localbinary_y_pred1 * 0.2 + hog_y_pred1 * 0.05)\n",
    "ensemble_model_weighted_averaging_accuracy = accuracy_score(Y_test,weighted_average_pred.argmax(axis=-1))\n",
    "print(\"The weighted averaging accuracy of the ensemble model is  = \", ensemble_model_weighted_averaging_accuracy)\n",
    "\n",
    "#plot confusion matrix\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test,weighted_average_pred.argmax(axis=-1),target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,weighted_average_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for Weighted Average Ensemble without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% save the predictions\n",
    "\n",
    "np.savetxt('weighted_averaging_y_pred.csv',weighted_average_pred.argmax(axis=-1),fmt='%i',delimiter = \",\")\n",
    "np.savetxt('Y_test_weighted_averaging.csv',Y_test,fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot roc curves\n",
    "\n",
    "wavg_fpr = dict()\n",
    "wavg_tpr = dict()\n",
    "wavg_roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    wavg_fpr[i], wavg_tpr[i], _ = roc_curve(Y_test, weighted_average_pred[:, i])\n",
    "    wavg_roc_auc[i] = auc(wavg_fpr[i], wavg_tpr[i])\n",
    "fig=plt.figure(figsize=(15,10), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "major_ticks = np.arange(0.0, 1.0, 0.1)\n",
    "minor_ticks = np.arange(0.0, 1.0, 0.1)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.grid(which='both')\n",
    "lw = 1 \n",
    "plt.plot(wavg_fpr[1], wavg_tpr[1], color='magenta',\n",
    "         lw=lw, label='ROC curve (area = %0.4f)' % wavg_roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristics for VGG19 model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate error\n",
    "ensemble_model_weighted_average_mean_squared_error = mean_squared_error(Y_test,weighted_average_pred.argmax(axis=-1))  \n",
    "ensemble_model_weighted_average_mean_squared_log_error = mean_squared_log_error(Y_test,weighted_average_pred.argmax(axis=-1))  \n",
    "print(\"The weighted averaging mean squared error of the ensemble model is  = \", ensemble_model_weighted_average_mean_squared_error)\n",
    "print(\"The weighted averaging mean squared log error of the ensemble model is  = \", ensemble_model_weighted_average_mean_squared_log_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model has its own weaknesses. The reasoning behind using an ensemble is that by stacking different models representing different hypotheses about the data, we can find a better hypothesis that is not in the hypothesis space of the models from which the ensemble is built. By using a very basic ensemble, a much lower error rate was achieved than when a single model was used. This proves effectiveness of ensembling. Of course, there are some practical considerations to keep in mind when using an ensemble for your machine learning task. Since ensembling means stacking multiple models together, it also means that the input data needs to be forward-propagated for each model. This increases the amount of compute that needs to be performed and, consequently, evaluation (predicition) time. Increased evaluation time is not critical if you use an ensemble in research or in a Kaggle competition. However, it is a very critical factor when designing a commercial product. Another consideration is increased size of the final model which, again, might be a limiting factor for ensemble use in a commercial product."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
